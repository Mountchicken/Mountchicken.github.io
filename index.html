<!DOCTYPE HTML>
<html>

<head>
	<title>Qing Jiang's Homepage</title>
	<link rel="icon" type="image/x-icon" href="assets/css/images/icon.png">
	<meta charset="utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
	<!-- <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Palatino:400,300,400,600,700" /> -->
	<link rel="stylesheet" href="assets/css/main.css" />
	<link rel="stylesheet" href="assets/css/dark-mode.css" />
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=G-ZVWWW5E2J3"></script>
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag() { dataLayer.push(arguments); }
		gtag('js', new Date());

		gtag('config', 'G-ZVWWW5E2J3');
	</script>
</head>

<body class="is-preload">

	<!-- Header -->
	<header id="header">
		<div class="inner">
			<a href="#" class="image avatar"><img src="images/avator.jpg" alt="" /></a>
			<h1>Qing Jiang (蒋擎)</h1>
			<h2>Ph.D. Candidate in SCUT</h2>
			<h2><a href="mountchicken@outlook.com">Email</a> | <a href="https://github.com/mountchiken">GitHub</a> | <a href="https://scholar.google.com/citations?user=fzIWn40AAAAJ&hl">Google Scholar</a> </h2>
			<h2>Object Detection, Vision Lanugage Understanding</h2>
		</div>
	</header>
	
	<!-- Main -->
	<div id="main">
		<nav id="nav">
			<ul>
				<li><a href="index.html">Home</a></li>
				<li><a href="blogs.html">Blogs</a></li>
				<li><a href="talks.html">Talks</a></li>
			</ul>
		</nav>
		<!-- One -->
		<section id="one">
			<header class="major">
				<h2>About Me</h2>
			</header>
			<p class="about-justify">
				I am now a second year Ph.D candidate at SCUT, under the supervision of Prof. <a href="https://www.leizhang.org/">Lei Zhang</a>.
				Currently I am having a long-term research internship at
				International Digital Economy Academy <a href="https://www.idea.edu.cn/">(IDEA)</a>.
				My research interests are focusd on Object Perception and Understanding. I am also dedicated
				to open source endeavors, which I believe is the fundamental element for the sustainable development of the AI community.
			</p>


			<!-- <h2>News</h2>
			<div class="row">
				<div class="col-12 col-12-xsmall">
					<ul>
						<li>[2024-] was accepted to NeurIPS (2023).</li>
						<li>One paper was accepted to ICLR (2023).</li>
						<li>One paper was accepted to ICASSP (2023).</li>
						<li>One paper was accepted to VLSI-SoC (2022), and won best paper nomination!!! </li> 
					</ul>
				</div>
			</div> -->

		</section>

		<!-- Preprint -->
		<section id="two">
			<h2>Preprint</h2>

			<div class="box alt">
				<article class="row gtr-50 gtr-uniform work-item">
					<div class="col-4">
						<a href="https://github.com/IDEA-Research/Rex-Omni">
							<video loop playsinline autoPlay muted
							src="images/publications/RexOmni.mp4" alt="SweetDreamer" style="width:105%; height:90%;"> </video>
						</a>
						<h3 hidden>A MLLM focused on perception tasks</h3>
					</div>
					<div class="col-8">
						<span>
							<h4>
								Detect Anything via Next Point Prediction
							</h4>
							<h5>
								<h5>
								<a href="https://mountchicken.github.io/"><strong>Qing Jiang</strong></a>,
								<a>Junan Huo</a>, 
								<a href="https://seanchenxy.github.io/">Xingyu Chen</a>, 
								<a>Yuda Xiong</a>,
								<a href="https://scholar.google.com/citations?hl=zh-CN&user=U_cvvUwAAAAJ">Zhaoyang Zeng</a>,
								<a>Yihao Chen</a>,
								<a href="https://rentainhe.github.io/">Tianhe Ren</a>, 
								<a>Junzhi Yu</a>
								<a href="https://leizhang.org/">Lei Zhang</a>,
								<br>
							<h5>
								[<a href="https://arxiv.org/abs/2510.12798">arXiv 2025</a>] | 
								[<a href="https://github.com/IDEA-Research/Rex-Omni">Github</a>]
								<a href="https://github.com/IDEA-Research/Rex-Omni"><img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/IDEA-Research/Rex-Omni" /> </a>
						</span>
						</h5>
						</span>
					</div>

				</article>
			</div>

			<div class="box alt">
				<article class="row gtr-50 gtr-uniform work-item">
					<div class="col-4">
						<a href="https://github.com/IDEA-Research/Rex-Thinker">
							<video loop playsinline autoPlay muted
							src="images/publications/Rex-Thinker.mp4" alt="SweetDreamer" style="width:105%; height:90%;"> </video>
						</a>
						<h3 hidden>Reasoning based object referring</h3>
					</div>
					<div class="col-8">
						<span>
							<h4>
								Rex-Thinker: Grounded Object Referring via Chain-of-Thought Reasoning
							</h4>
							<h5>
								<h5>
								<a href="https://mountchicken.github.io/"><strong>Qing Jiang*</strong></a>,
								<a href="https://seanchenxy.github.io/">Xingyu Chen*</a>, 
								<a href="https://scholar.google.com/citations?hl=zh-CN&user=U_cvvUwAAAAJ">Zhaoyang Zeng</a>,
								<a>Junzhi Yu</a>, 
								<a href="https://leizhang.org/">Lei Zhang</a>,
								<br>
							<h5>
								[<a href="https://arxiv.org/abs/2506.04034">arXiv 2025</a>] | 
								[<a href="https://github.com/IDEA-Research/Rex-Thinker">Github</a>]
								<a href="https://github.com/IDEA-Research/Rex-Thinker"><img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/IDEA-Research/Rex-Thinker" /> </a>
						</span>
						</h5>
						</span>
					</div>

				</article>
			</div>
			

			<div class="box alt">
				<article class="row gtr-50 gtr-uniform work-item">
					<div class="col-4">
						<a href="https://github.com/IDEA-Research/ChatRex">
							<video loop playsinline autoPlay muted
							src="images/publications/chatrex.mp4" alt="SweetDreamer" style="width:105%; height:90%;"> </video>
						</a>
						<h3 hidden>A MLLM focused on perception tasks</h3>
					</div>
					<div class="col-8">
						<span>
							<h4>
								ChatRex: Tamming Multimodal LLM for Joint Perception and Understanding
							</h4>
							<h5>
								<h5>
								<a href="https://mountchicken.github.io/"><strong>Qing Jiang</strong></a>,
								<a href="https://github.com/luogen1996">Gen Luo</a>, 
								<a>Yuqin Yang</a>, 
								<a>Yihao Chen</a>,
								<a>Yuda Xiong</a>,
								<a href="https://scholar.google.com/citations?hl=zh-CN&user=U_cvvUwAAAAJ">Zhaoyang Zeng*</a>,
								<a href="https://rentainhe.github.io/">Tianhe Ren*</a>, 
								<a href="https://leizhang.org/">Lei Zhang</a>,
								<br>
							<h5>
								[<a href="https://arxiv.org/abs/2411.18363">arXiv 2024</a>] | 
								[<a href="https://github.com/IDEA-Research/ChatRex">Github</a>]
								<a href="https://github.com/IDEA-Research/ChatRex"><img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/IDEA-Research/ChatRex" /> </a>
						</span>
						</h5>
						</span>
					</div>

				</article>
			</div>

			<div class="box alt">
				<article class="row gtr-50 gtr-uniform work-item">
					<div class="col-4">
						<a href="https://github.com/IDEA-Research/T-Rex">
							<video loop playsinline autoPlay muted
							src="images/publications/DINOX.mp4" alt="SweetDreamer" style="width:105%; height:90%;"> </video>
						</a>
						<h3 hidden>Interactive and Promptable Model for Open-set Object Detection</h3>
					</div>
					<div class="col-8">
						<span>
							<h4>
								DINO-X: A Unified Vision Model for Open-World Object Detection and Understanding
							</h4>
							<h5>
								<a href="https://mountchicken.github.io/"><strong>IDEA-CVR Team</strong></a>
								<br>
							</h5>
							<h5>
								[<a href="https://arxiv.org/abs/2411.14347">arXiv 2024</a>] | 
								[<a href="https://deepdataspace.com/blog/7">Homepage</a>] | [<a href="https://github.com/IDEA-Research/DINO-X-API">Github</a>] | [<a href="https://deepdataspace.com/playground/dino-x/">Demo</a>] |
								<a href="https://github.com/IDEA-Research/DINO-X-API"><img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/IDEA-Research/DINO-X-API" /> </a>
							</span>
						</h5>
						</span>
					</div>

				</article>
			</div>

			<div class="box alt">
				<article class="row gtr-50 gtr-uniform work-item">
					<div class="col-4">
						<a href="https://github.com/IDEA-Research/Grounding-DINO-1.5-API">
							<video loop playsinline autoPlay muted
							src="images/publications/GroundingDINO1.5.mp4" alt="SweetDreamer" style="width:105%; height:90%;"> </video>
						</a>
						<h3 hidden>IDEA Research's Most Capable Open-World Object Detection Model Series</h3>
					</div>
					<div class="col-8">
						<span>
							<h4>
								Grounding DINO 1.5: Advance the "Edge" of Open-Set Object Detection
							</h4>
							<h5>
								<a href="https://rentainhe.github.io/">Tianhe Ren*</a>,
								<a href="https://mountchicken.github.io/"><strong>Qing Jiang*</strong></a>,
								<a href="https://www.lsl.zone/">Shilong Liu*</a>, 
								<a href="https://scholar.google.com/citations?hl=zh-CN&user=U_cvvUwAAAAJ">Zhaoyang Zeng*</a>, 
								<a>Wenlong Liu</a>,
								<a>Han Gao</a>,
								<a>Hongjie Huang</a>,
								<a>Zhengyu Ma</a>,
								<a>Xiaoke Jiang</a>,
								<a>Yihao Chen</a>,
								<a>Yuda Xiong</a>,
								<a href="https://haozhang534.github.io/">Hao Zhang</a>,
								<a href="https://fengli-ust.github.io/">Feng Li</a>,
								<a>Peijun Tang</a>,
								<a>Kent Yu</a>,
								<a href="https://leizhang.org/">Lei Zhang</a>,
								<br>
							</h5>
							<h5>
								[<a href="https://arxiv.org/abs/2405.10300">arXiv Preprint 2024</a>] | 
								[<a href="https://deepdataspace.com/home">Homepage</a>] | [<a href="https://github.com/IDEA-Research/Grounding-DINO-1.5-API">Github</a>] | [<a href="https://deepdataspace.com/playground/grounding_dino">Demo</a>]
								<a href="https://github.com/IDEA-Research/Grounding-DINO-1.5-API"><img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/IDEA-Research/Grounding-DINO-1.5-API" /> </a>
							</span>
						</h5>
						</span>
					</div>

				</article>
			</div>

			
			<div class="box alt">
				<article class="row gtr-50 gtr-uniform work-item">
					<!-- <div class="col-4">
						<a href="images/humantomato.gif">
							<img
								src="images/humantomato.gif" alt="EG2022_MoCo-Flow" >
						</a>
						<h3 hidden>We present the <b>FIRST</b> attempt to generate whole-body motions with text description.</h3>
					</div> -->
					<div class="col-4">
						<a href="https://github.com/IDEA-Research/T-Rex/tree/main">
							<video loop playsinline autoPlay muted
							src="images/publications/T-Rex.mp4" alt="SweetDreamer" style="width:105%; height:90%;"> </video>
						</a>
						<h3 hidden>Interactive Object Counting Model</h3>
					</div>
					<div class="col-8">
						<span>
							<h4>
								T-Rex: Counting by Visual Prompting
							</h4>
							<h5>
								<a href="https://mountchicken.github.io/"><strong>Qing Jiang</strong></a>,
								<a href="https://fengli-ust.github.io/">Feng Li</a>, 
								<a href="https://rentainhe.github.io/">Tianhe Ren</a>, 
								<a href="https://www.lsl.zone/">Shilong Liu</a>,
								<a href="https://scholar.google.com/citations?hl=zh-CN&user=U_cvvUwAAAAJ">Zhaoyang Zeng</a>,
								<a>Kent Yu</a>,
								<a href="https://leizhang.org/">Lei Zhang</a>,
								<br>
							</h5>
							<h5>
								[<a href="https://arxiv.org/abs/2311.13596">arXiv Preprint 2024</a>] | 
								[<a href="https://trex-counting.github.io/">Homepage</a>] | [<a href="https://github.com/IDEA-Research/T-Rex/tree/main">Github</a>] | [<a href="https://deepdataspace.com/playground/ivp">Demo</a>]
								<a href="https://github.com/IDEA-Research/T-Rex"><img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/IDEA-Research/T-Rex" /> </a>
							</span>
						</h5>
						</span>
					</div>

				</article>
			</div>

		</section>

		<!-- Publications -->
		<section id="two">
			<h2>Publications</h2>

			<div class="box alt">
				<article class="row gtr-50 gtr-uniform work-item">
					<div class="col-4">
						<a href="https://github.com/IDEA-Research/RexSeek">
							<video loop playsinline autoPlay muted
							src="images/publications/RexSeek.mp4" alt="SweetDreamer" style="width:105%; height:90%;"> </video>
						</a>
						<h3 hidden>A MLLM focused on perception tasks</h3>
					</div>
					<div class="col-8">
						<span>
							<h4>
								Referring to Any Person
							</h4>
							<h5>
								<h5>
								<a href="https://mountchicken.github.io/"><strong>Qing Jiang</strong></a>,
								<a>Lin Wu</a>, 
								<a href="https://scholar.google.com/citations?hl=zh-CN&user=U_cvvUwAAAAJ">Zhaoyang Zeng</a>,
								<a>Tianhe Ren</a>,
								<a>Yuda Xiong</a>,
								<a>Yihao Chen</a>, 
								<a href="https://leizhang.org/">Lei Zhang</a>,
								<br>
							<h5>
								[<a href="https://arxiv.org/abs/2503.08507">ICCV 2025</a>] | 
								[<a href="https://github.com/IDEA-Research/RexSeek">Github</a>]
								<a href="https://github.com/IDEA-Research/RexSeek"><img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/IDEA-Research/RexSeek" /> </a>
						</span>
						</h5>
						</span>
					</div>

				</article>
			</div>

			<div class="box alt">
				<article class="row gtr-50 gtr-uniform work-item">
					<div class="col-4">
						<a href="https://github.com/IDEA-Research/T-Rex">
							<video loop playsinline autoPlay muted
							src="images/publications/T-Rex2.mov" alt="SweetDreamer" style="width:105%; height:90%;"> </video>
						</a>
						<h3 hidden>Interactive and Promptable Model for Open-set Object Detection</h3>
					</div>
					<div class="col-8">
						<span>
							<h4>
								T-Rex2: Towards Generic Object Detection via Text-Visual Prompt Synergy
							</h4>
							<h5>
								<a href="https://mountchicken.github.io/"><strong>Qing Jiang</strong></a>,
								<a href="https://fengli-ust.github.io/">Feng Li</a>, 
								<a href="https://scholar.google.com/citations?hl=zh-CN&user=U_cvvUwAAAAJ">Zhaoyang Zeng</a>, 
								<a href="https://rentainhe.github.io/">Tianhe Ren</a>, 
								<a href="https://www.lsl.zone/">Shilong Liu</a>, 
								<a href="https://leizhang.org/">Lei Zhang</a>.
								<br>
							</h5>
							<h5>
								[<a href="https://arxiv.org/abs/2403.14610">ECCV 2024</a>] | 
								[<a href="https://deepdataspace.com/home">Homepage</a>] | [<a href="https://github.com/IDEA-Research/T-Rex">Github</a>] | [<a href="https://deepdataspace.com/playground/ivp">Demo</a>] |
								<a href="https://github.com/IDEA-Research/T-Rex"><img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/IDEA-Research/T-Rex" /> </a>
						</span>
						</h5>
						</span>
					</div>

				</article>
			</div>

			<div class="box alt">
				<article class="row gtr-50 gtr-uniform work-item">
					<div class="col-4">
						<a href="https://arxiv.org/pdf/2311.13601.pdf" data-poptrox="youtube" class="video fit thumb">
							<img src="images/publications/dinov.png" alt="vlsi" style="width:100%; height:80%;" > 
						</a>
						<h3 hidden>Visual Promptable Object Detection and Segmentation</h3>
					</div>
					<div class="col-8">
						<span>
							<h4>
								Visual In-Context Prompting
							</h4>
							<h5>
								<a href="https://fengli-ust.github.io/">Feng Li</a>, 
								<a href="https://mountchicken.github.io/"><strong>Qing Jiang</strong></a>,
								<a>Hao Zhang</a>,
								<a>Tianhe Ren</a>,
								<a>Shilong Liu</a>,
								<a>Xueyan Zou</a>,
								<a>Huaizhe Xu</a>,
								<a>Hongyang Li</a>
								<a>Chunyuan Li</a>
								<a>jianwei Yang</a>
								<a>Lei Zhang</a>
								<a>Jianfeng Gao</a>
							</h5>
							<h5>
								[<a href="https://arxiv.org/pdf/2311.13601.pdf">CVPR 2024</a>] |
								[<a href="https://github.com/UX-Decoder/DINOv">Code</a>] |
								<a href="https://github.com/UX-Decoder/DINOv"><img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/UX-Decoder/DINOv" /> </a>
						</span>
						</h5>
						</span>
					</div>

				</article>
			</div>

			<!-- <hr style="height:1px;border:none;border-top:1px dashed #0066CC;"/> -->

			<div class="box alt">
				<article class="row gtr-50 gtr-uniform work-item">
					<div class="col-4">
						<a href="https://arxiv.org/abs/2307.08723" data-poptrox="youtube" class="video fit thumb">
							<img src="images/publications/union14m.png" alt="iccv" style="width:100%; height:80%;" > 
						</a>
					</div>
					<div class="col-8">
						<span>
							<h4>
								Revisiting Scene Text Recognition: A Data Perspective
							</h4>
							<h5>
								<a href="https://mountchicken.github.io/"><strong>Qing Jiang</strong></a>,
								<a>Jiapeng Wang</a>,
								<a>Dezhi Peng</a>,
								<a>Chongyu Liu</a>,
								<a>Lianwen Jin</a>
							</h5>
							<h5>
								[<a href="https://arxiv.org/abs/2307.08723">ICCV 2023</a>] | 
								[<a href="https://union14m.github.io/">Homepage</a>] | [<a href="https://github.com/Mountchicken/Union14M">Code</a>] | [<a href="https://huggingface.co/spaces/Mountchicken/MAERec-Gradio">Demo</a>] |
								<a href="https://github.com/Mountchicken/Union14M"><img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/Mountchicken/Union14M" /> </a>
						</span>
						</h5>
						</span>
					</div>

				</article>
			</div>

		</section>

		<!-- Products -->
		<section id="two">
			<h2>Products</h2>
			
			<div class="box alt">
				<article class="row gtr-50 gtr-uniform work-item">
					<div class="col-4">
						<a href="https://apps.apple.com/us/app/countanything/id6502489882">
							<video loop playsinline autoPlay muted
							src="images/products/countanything.mov" alt="SweetDreamer" style="width:105%; height:90%;"> </video>
						</a>
					</div>
					<div class="col-8">
						<span>
							<h4>
								CountAnything: Powerful Counting APP on IOS
							</h4>
							<h5>
								CountAnything is a cutting-edge counting application that leverages advanced computer vision algorithms to provide automatic
								counting capabilities. Whether you're in the industrial, agricultural, or aquaculture sectors, or simply have counting needs,
								CountAnything makes the process effortless and accurate.
							</h5>
						</span>
						</h5>
						</span>
					</div>

				</article>
			</div>
			
			<div class="box alt">
				<article class="row gtr-50 gtr-uniform work-item">
					<div class="col-4">
						<a href="https://www.trexlabel.com/?source=gh">
							<video loop playsinline autoPlay muted
							src="images/products/T-RexLabel.mov" alt="SweetDreamer" style="width:105%; height:90%;"> </video>
						</a>
					</div>
					<div class="col-8">
						<span>
							<h4>
								T-Rex Label: Intelligent online annotation tool
							</h4>
							<h5>
								T-Rex Label is an intelligent tool designed for complex scenarios annotation, applicable across various industries.
								It is the go-to option for those aiming to streamline their workflows and effortlessly create high-quality datasets.
							</h5>
						</span>
					</div>

				</article>
			</div>


		</section>

		<!-- Open Source -->
		<section id="two">
			<h2>Open Source</h2>

			<div class="box alt">
				<article class="row gtr-50 gtr-uniform work-item">
					<div class="col-4">
						<a href="blogs/blog1.html" data-poptrox="youtube" class="video fit thumb">
							<img src="images/open_source/cookbook.jpg" alt="vlsi" style="width:100%; height:80%;" > 
						</a>
						<h3 hidden>Cookbook to Craft Good Code</h3>
					</div>
					<div class="col-8">
						<span>
							<h4>
								Cookbook to Craft Good Code
							</h4>
							<h5>
								In this guide, we'll dive into the essentials of crafting great code. We'll go through everything from how to name things clearly and highlight tools that make coding better and easier.
							</h5>
							<h5>
								<a href="https://github.com/Mountchicken/CodeCookbook"><img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/Mountchicken/CodeCookbook" /> </a>
						</span>
						</h5>
						</span>
					</div>

				</article>
			</div>

			<div class="box alt">
				<article class="row gtr-50 gtr-uniform work-item">
					<div class="col-4">
						<a href="https://github.com/open-mmlab/mmocr" data-poptrox="youtube" class="video fit thumb">
							<img src="images/open_source/mmocr-logo.png" alt="vlsi" style="width:100%; height:80%;" > 
						</a>
						<h3 hidden>MMOCR</h3>
					</div>
					<div class="col-8">
						<span>
							<h4>
								MMOCR
							</h4>
							<h5>
								OpenMMLab Text Detection, Recognition and Understanding Toolbox.
							</h5>
							<h5>
								<a href="https://github.com/open-mmlab/mmocr"><img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/open-mmlab/mmocr" /> </a>
						</span>
						</h5>
						</span>
					</div>

				</article>
			</div>

			<div class="box alt">
				<article class="row gtr-50 gtr-uniform work-item">
					<div class="col-4">
						<a href="https://github.com/HCIILAB/Scene-Text-Recognition-Recommendations" data-poptrox="youtube" class="video fit thumb">
							<img src="images/open_source/strr.jpeg" alt="vlsi" style="width:100%; height:80%;" > 
						</a>
						<h3 hidden>MMOCR</h3>
					</div>
					<div class="col-8">
						<span>
							<h4>
								Scene Text Recognition Recommendations
							</h4>
							<h5>
								Long-time maintaining project for recording latest papers, datasets, algorithms, and SOTAs for
                    scene text recognition
							</h5>
							<h5>
								<a href="https://github.com/HCIILAB/Scene-Text-Recognition-Recommendations"><img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/HCIILAB/Scene-Text-Recognition-Recommendations" /> </a>
						</span>
						</h5>
						</span>
					</div>

				</article>
			</div>

			<div class="box alt">
				<article class="row gtr-50 gtr-uniform work-item">
					<div class="col-4">
						<a href="https://github.com/yeungchenwa/OCR-SAM" data-poptrox="youtube" class="video fit thumb">
							<img src="images/open_source/ocrsam.png" alt="vlsi" style="width:100%; height:80%;" > 
						</a>
						<h3 hidden>OCR-SAM</h3>
					</div>
					<div class="col-8">
						<span>
							<h4>
								OCR-SAM
							</h4>
							<h5>
								Combining MMOCR with Segment Anything & Stable Diffusion. Automatically detect, recognize and
                    segment text instances, with serval downstream tasks, e.g., Text Removal and Text Inpainting
							</h5>
							<h5>
								<a href="https://github.com/yeungchenwa/OCR-SAM"><img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/yeungchenwa/OCR-SAM" /> </a>
						</span>
						</h5>
						</span>
					</div>

				</article>
			</div>

			<div class="box alt">
				<article class="row gtr-50 gtr-uniform work-item">
					<div class="col-4">
						<a href="https://github.com/Mountchicken/Efficient-Deep-Learning" data-poptrox="youtube" class="video fit thumb">
							<img src="images/open_source/edl.png" alt="vlsi" style="width:100%; height:80%;" > 
						</a>
						<h3 hidden>Efficient Deep Learning</h3>
					</div>
					<div class="col-8">
						<span>
							<h4>
								Efficient Deep Learning
							</h4>
							<h5>
								Combining MMOCR with Segment Anything & Stable Diffusion. Automatically detect, recognize and
                    segment text instances, with serval downstream tasks, e.g., Text Removal and Text Inpainting
							</h5>
							<h5>
								<a href="https://github.com/Mountchicken/Efficient-Deep-Learning"><img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/Mountchicken/Efficient-Deep-Learning" /> </a>
						</span>
						</h5>
						</span>
					</div>

				</article>
			</div>

			<div class="box alt">
				<article class="row gtr-50 gtr-uniform work-item">
					<div class="col-4">
						<a href="https://github.com/Mountchicken/Text-Recognition-on-Cross-Domain-Datasets" data-poptrox="youtube" class="video fit thumb">
							<img src="images/open_source/trcdd.png" alt="vlsi" style="width:100%; height:80%;" > 
						</a>
						<h3 hidden>Text Recognition on Cross Domain Datasets</h3>
					</div>
					<div class="col-8">
						<span>
							<h4>
								Text Recognition on Cross Domain Datasets
							</h4>
							<h5>
								Improved Text recognition algorithms on different text domains like scene text, handwritten,
                    document, Chinese/English
							</h5>
							<h5>
								<a href="https://github.com/Mountchicken/Text-Recognition-on-Cross-Domain-Datasets"><img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/Mountchicken/Text-Recognition-on-Cross-Domain-Datasets" /> </a>
						</span>
						</h5>
						</span>
					</div>

				</article>
			</div>

			<div class="box alt">
				<article class="row gtr-50 gtr-uniform work-item">
					<div class="col-4">
						<a href="https://github.com/Mountchicken/Structured_Dreambooth_LoRA" data-poptrox="youtube" class="video fit thumb">
							<img src="images/open_source/lora.png" alt="vlsi" style="width:100%; height:80%;" > 
						</a>
						<h3 hidden>Structured Dreambooth LoRA</h3>
					</div>
					<div class="col-8">
						<span>
							<h4>
								Structured Dreambooth LoRA
							</h4>
							<h5>
								Dreambooth (LoRA) with well-organized code structure. Naive adaptation from Diffusers.
							</h5>
							<h5>
								<a href="https://github.com/Mountchicken/Structured_Dreambooth_LoRA"><img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/Mountchicken/Structured_Dreambooth_LoRA" /> </a>
						</span>
						</h5>
						</span>
					</div>

				</article>
			</div>
		</section>



		<!-- Three -->
		<section id="three">
			<h2>Experience</h2>
			<div class="table-wrapper">
				<table>
					<tbody>
						<tr>
							<td><b>International Digital Economy Academy (IDEA)</b> | Research intern</td>
							<td>2023.06 – now</td>
						</tr>
						<tr>
							<td><b>Shanghai AI Lab (OpenMMLab)</b> | Intern</td>
							<td>2022.02 – 2022.08</td>
						</tr>
					</tbody>
				</table>
			</div>
			
		</section>
	</div>


	
	<!-- Footer -->
	<footer id="footer">
		<div class="inner">
			<ul class="icons">
				<li><a href="mountchicken@outlook.com" class="icon solid fa-envelope"><span
					class="label">Email</span></a></li>
				<li><a href="https://twitter.com/mountch1cken" class="icon brands fa-twitter"><span
							class="label">Twitter</span></a></li>
				<li><a href="https://github.com/mountchicken" class="icon brands fa-github"><span
							class="label">Github</span></a></li>
				<li><a href="https://scholar.google.com/citations?user=fzIWn40AAAAJ&hl=zh-CN" class="icon solid fa-graduation-cap"><span 
							class="label">GoogleScholar</span></a></li> 
			</ul>
			<ul class="copyright">
				<li>&copy; Qing Jiang. All rights reserved.</li>
				<li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
			</ul>
		</div>
	</footer>

	<!-- Scripts -->
	<script src="assets/js/jquery.min.js"></script>
	<script src="assets/js/jquery.poptrox.min.js"></script>
	<script src="assets/js/browser.min.js"></script>
	<script src="assets/js/breakpoints.min.js"></script>
	<script src="assets/js/util.js"></script>
	<script src="assets/js/main.js"></script>
	<script src="assets/js/dark-mode.js"></script>

</body>

</html>
